{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNeW4NFcG7mn6Y+ppAmZy0N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 1. 모델을 클래스로 구현하기\n","### 1.1 단순 선형 회귀"],"metadata":{"id":"d7X-VgpZ3-BD"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tl-P8Tca307T","executionInfo":{"status":"ok","timestamp":1700033474162,"user_tz":-540,"elapsed":8400,"user":{"displayName":"한지석","userId":"02284980391702693291"}},"outputId":"aa44e762-91fd-4b74-ac03-50e408003db2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch    0/2000 Cost: 13.103541\n","Epoch  100/2000 Cost: 0.002791\n","Epoch  200/2000 Cost: 0.001724\n","Epoch  300/2000 Cost: 0.001066\n","Epoch  400/2000 Cost: 0.000658\n","Epoch  500/2000 Cost: 0.000407\n","Epoch  600/2000 Cost: 0.000251\n","Epoch  700/2000 Cost: 0.000155\n","Epoch  800/2000 Cost: 0.000096\n","Epoch  900/2000 Cost: 0.000059\n","Epoch 1000/2000 Cost: 0.000037\n","Epoch 1100/2000 Cost: 0.000023\n","Epoch 1200/2000 Cost: 0.000014\n","Epoch 1300/2000 Cost: 0.000009\n","Epoch 1400/2000 Cost: 0.000005\n","Epoch 1500/2000 Cost: 0.000003\n","Epoch 1600/2000 Cost: 0.000002\n","Epoch 1700/2000 Cost: 0.000001\n","Epoch 1800/2000 Cost: 0.000001\n","Epoch 1900/2000 Cost: 0.000000\n","Epoch 2000/2000 Cost: 0.000000\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","torch.manual_seed(1)\n","\n","# 데이터\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[2], [4], [6]])\n","\n","class LinearRegressionModel(nn.Module): # torch.nn.Module을 상속받는 파이썬 클래스\n","    def __init__(self): #\n","        super().__init__()\n","        self.linear = nn.Linear(1, 1) # 단순 선형 회귀이므로 input_dim=1, output_dim=1.\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","\n","model = LinearRegressionModel()\n","\n","# optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","\n","# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n","nb_epochs = 2000\n","for epoch in range(nb_epochs+1):\n","\n","    # H(x) 계산\n","    prediction = model(x_train)\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n","\n","    # cost로 H(x) 개선하는 부분\n","    # gradient를 0으로 초기화\n","    optimizer.zero_grad()\n","    # 비용 함수를 미분하여 gradient 계산\n","    cost.backward() # backward 연산\n","    # W와 b를 업데이트\n","    optimizer.step()\n","\n","    if epoch % 100 == 0:\n","    # 100번마다 로그 출력\n","      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","          epoch, nb_epochs, cost.item()\n","      ))\n","\n"]},{"cell_type":"code","source":["# 임의의 입력 4를 선언\n","new_var =  torch.FloatTensor([[4.0]])\n","# 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n","pred_y = model(new_var) # forward 연산\n","# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것\n","print(\"훈련 후 입력이 4일 때의 예측값 :\", pred_y)\n","# W, b\n","print(list(model.parameters()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"koCuWiGI5DC9","executionInfo":{"status":"ok","timestamp":1700033479280,"user_tz":-540,"elapsed":517,"user":{"displayName":"한지석","userId":"02284980391702693291"}},"outputId":"66f71193-1e7d-49d8-d2bd-f2c23dc13731"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 후 입력이 4일 때의 예측값 : tensor([[7.9989]], grad_fn=<AddmmBackward0>)\n","[Parameter containing:\n","tensor([[1.9994]], requires_grad=True), Parameter containing:\n","tensor([0.0014], requires_grad=True)]\n"]}]},{"cell_type":"markdown","source":["### 1.2 다중 선형 회귀"],"metadata":{"id":"f_J-LOhQ5P5b"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","torch.manual_seed(1)\n","\n","# 데이터\n","x_train = torch.FloatTensor([[73, 80, 75],\n","                             [93, 88, 93],\n","                             [89, 91, 90],\n","                             [96, 98, 100],\n","                             [73, 66, 70]])\n","y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n","\n","class MultivariateLinearRegressionModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear = nn.Linear(3, 1) # 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","\n","model = MultivariateLinearRegressionModel()\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n","\n","nb_epochs = 2000\n","for epoch in range(nb_epochs+1):\n","\n","    # H(x) 계산\n","    prediction = model(x_train)\n","    # model(x_train)은 model.forward(x_train)와 동일함.\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n","\n","    # cost로 H(x) 개선하는 부분\n","    # gradient를 0으로 초기화\n","    optimizer.zero_grad()\n","    # 비용 함수를 미분하여 gradient 계산\n","    cost.backward()\n","    # W와 b를 업데이트\n","    optimizer.step()\n","\n","    if epoch % 100 == 0:\n","    # 100번마다 로그 출력\n","      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","          epoch, nb_epochs, cost.item()\n","      ))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J-2cDbq55Equ","executionInfo":{"status":"ok","timestamp":1700033563236,"user_tz":-540,"elapsed":1091,"user":{"displayName":"한지석","userId":"02284980391702693291"}},"outputId":"b3d8a9c1-94df-4fa3-dfae-4a93d214180c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch    0/2000 Cost: 31667.597656\n","Epoch  100/2000 Cost: 0.225993\n","Epoch  200/2000 Cost: 0.223911\n","Epoch  300/2000 Cost: 0.221941\n","Epoch  400/2000 Cost: 0.220059\n","Epoch  500/2000 Cost: 0.218271\n","Epoch  600/2000 Cost: 0.216575\n","Epoch  700/2000 Cost: 0.214950\n","Epoch  800/2000 Cost: 0.213413\n","Epoch  900/2000 Cost: 0.211952\n","Epoch 1000/2000 Cost: 0.210560\n","Epoch 1100/2000 Cost: 0.209232\n","Epoch 1200/2000 Cost: 0.207967\n","Epoch 1300/2000 Cost: 0.206761\n","Epoch 1400/2000 Cost: 0.205619\n","Epoch 1500/2000 Cost: 0.204522\n","Epoch 1600/2000 Cost: 0.203484\n","Epoch 1700/2000 Cost: 0.202485\n","Epoch 1800/2000 Cost: 0.201542\n","Epoch 1900/2000 Cost: 0.200635\n","Epoch 2000/2000 Cost: 0.199769\n"]}]}]}